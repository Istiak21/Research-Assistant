{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a06b4c4-87e5-4477-a677-3b0690b1368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "============= AI RESEARCH ASSISTANT ==============\n",
      "==================================================\n",
      "\n",
      "I can help research any topic and generate a detailed report.\n",
      "Just tell me what you'd like to research!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/istiak/anaconda3/lib/python3.12/site-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_6901/381316319.py:64: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  return self.copy(update={**kwargs, **({\"stop\": stop} if stop else {})})\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What would you like to research? (or 'quit' to exit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a more specific topic (at least 2 words).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What would you like to research? (or 'quit' to exit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a more specific topic (at least 2 words).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What would you like to research? (or 'quit' to exit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thank you for using the AI Research Assistant!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.tools import tool\n",
    "from together import Together\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import LLMResult, Generation\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Together AI client\n",
    "client = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "class TogetherAILLM(BaseLLM):\n",
    "    \"\"\"LangChain LLM wrapper for Together AI\"\"\"\n",
    "    \n",
    "    model: str = \"meta-llama/Llama-3-8b-chat-hf\"\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 2048\n",
    "    \n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Any = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=self.max_tokens,\n",
    "                    stop=stop,\n",
    "                    **kwargs\n",
    "                )\n",
    "                text = response.choices[0].message.content\n",
    "                generations.append([Generation(text=text)])\n",
    "            except Exception as e:\n",
    "                if run_manager:\n",
    "                    run_manager.on_llm_error(e)\n",
    "                raise\n",
    "        \n",
    "        return LLMResult(generations=generations)\n",
    "    \n",
    "    def _llm_type(self) -> str:\n",
    "        return \"together_ai\"\n",
    "    \n",
    "    def bind(self, stop: Optional[List[str]] = None, **kwargs: Any) -> BaseLLM:\n",
    "        return self.copy(update={**kwargs, **({\"stop\": stop} if stop else {})})\n",
    "\n",
    "# Initialize LLM\n",
    "llm = TogetherAILLM()\n",
    "\n",
    "@tool\n",
    "def chrome_research(query: str, max_results: int = 3) -> str:\n",
    "    \"\"\"Perform web research using Chrome browser.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(f\"https://www.google.com/search?q={query}\")\n",
    "        time.sleep(2)\n",
    "        results = []\n",
    "        links = driver.find_elements(By.CSS_SELECTOR, \"div.g a\")[:max_results]\n",
    "        \n",
    "        for i, link in enumerate(links):\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and href.startswith(\"http\"):\n",
    "                driver.execute_script(f\"window.open('{href}');\")\n",
    "                driver.switch_to.window(driver.window_handles[i+1])\n",
    "                time.sleep(2)\n",
    "                \n",
    "                try:\n",
    "                    visible_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "                    results.append(visible_text[:5000])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting text from {href}: {str(e)}\")\n",
    "                \n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "        \n",
    "        return \"\\n\\n\".join(results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during research: {str(e)}\")\n",
    "        return f\"Research failed: {str(e)}\"\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def setup_agent():\n",
    "    \"\"\"Set up the research agent and executor\"\"\"\n",
    "    tools = [chrome_research]\n",
    "    research_prompt = hub.pull(\"hwchase17/react\")\n",
    "    research_agent = create_react_agent(llm, tools, research_prompt)\n",
    "    return AgentExecutor(agent=research_agent, tools=tools, verbose=True)\n",
    "\n",
    "def get_user_topic():\n",
    "    \"\"\"Get research topic from user with validation\"\"\"\n",
    "    while True:\n",
    "        topic = input(\"\\nWhat would you like to research? (or 'quit' to exit): \").strip()\n",
    "        if topic.lower() == 'quit':\n",
    "            return None\n",
    "        if len(topic.split()) < 2:\n",
    "            print(\"Please enter a more specific topic (at least 2 words).\")\n",
    "            continue\n",
    "        if len(topic) > 200:\n",
    "            print(\"Topic too long. Please keep it under 200 characters.\")\n",
    "            continue\n",
    "        return topic\n",
    "\n",
    "def research_workflow(topic: str, research_executor: AgentExecutor):\n",
    "    \"\"\"Execute the full research workflow for a given topic\"\"\"\n",
    "    print(f\"\\nStarting research on: {topic}\")\n",
    "    \n",
    "    try:\n",
    "        # Generate research questions\n",
    "        questions = llm.invoke(f\"Generate 3 specific research questions about {topic}\")\n",
    "        questions = [q.strip() for q in questions.split(\"\\n\") if q.strip()][:3]\n",
    "        \n",
    "        # Conduct research\n",
    "        findings = []\n",
    "        for question in questions:\n",
    "            try:\n",
    "                print(f\"\\nResearching: {question}\")\n",
    "                research = research_executor.invoke({\"input\": question})\n",
    "                findings.append(research[\"output\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error researching {question}: {str(e)}\")\n",
    "                findings.append(f\"Failed to research: {question}\")\n",
    "        \n",
    "        # Generate report\n",
    "        report = llm.invoke(\n",
    "            f\"Summarize these research findings about {topic}:\\n{'\\n'.join(findings)}\\n\\n\"\n",
    "            \"Provide a well-structured report with key points and sources.\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"questions\": questions,\n",
    "            \"findings\": findings,\n",
    "            \"report\": report\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in research workflow: {str(e)}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"questions\": [],\n",
    "            \"findings\": [],\n",
    "            \"report\": \"\"\n",
    "        }\n",
    "\n",
    "def display_results(results: dict):\n",
    "    \"\"\"Display research results in a user-friendly format\"\"\"\n",
    "    if \"error\" in results:\n",
    "        print(\"\\nWorkflow failed with error:\", results[\"error\"])\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" RESEARCH RESULTS \".center(50, \"=\"))\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nRESEARCH QUESTIONS:\")\n",
    "    for i, question in enumerate(results[\"questions\"], 1):\n",
    "        print(f\"{i}. {question}\")\n",
    "    \n",
    "    print(\"\\nFINAL REPORT:\")\n",
    "    print(results[\"report\"])\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main interactive research loop\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" AI RESEARCH ASSISTANT \".center(50, \"=\"))\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nI can help research any topic and generate a detailed report.\")\n",
    "    print(\"Just tell me what you'd like to research!\\n\")\n",
    "    \n",
    "    research_executor = setup_agent()\n",
    "    \n",
    "    while True:\n",
    "        topic = get_user_topic()\n",
    "        if not topic:\n",
    "            break\n",
    "            \n",
    "        results = research_workflow(topic, research_executor)\n",
    "        display_results(results)\n",
    "        \n",
    "        continue_research = input(\"\\nWould you like to research another topic? (y/n): \").strip().lower()\n",
    "        if continue_research != 'y':\n",
    "            break\n",
    "    \n",
    "    print(\"\\nThank you for using the AI Research Assistant!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8b8c6-06a9-48ef-a5a6-0d5bd16a95d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
